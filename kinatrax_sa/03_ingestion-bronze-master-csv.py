# Databricks notebook source
# MAGIC %run ./00_ddl

# COMMAND ----------

# MAGIC %run ./01_imports

# COMMAND ----------

# Imports have already been set in the 00_setup notebook
# Widgets and task values
dbutils.widgets.text(name='TABLE', defaultValue='', label='Table')
dbutils.widgets.text(name='EVENT_TYPE', defaultValue='', label='Event Type')
dbutils.widgets.text(name='DELIMITER', defaultValue=' ', label='Delimiter')

# Pipeline and task variables
TABLE = dbutils.widgets.get('TABLE')
EVENT_TYPE = dbutils.widgets.get('EVENT_TYPE')
DELIMITER = dbutils.widgets.get('DELIMITER')
print(f"Table: {TABLE}")
print(f"Event Type: {EVENT_TYPE}")
print(f"Delimiter: {DELIMITER}")

# Path variables
table_bronze = f'{CATALOG}.{DATABASE_B}.{EVENT_TYPE}_{TABLE}'
schema_location_bronze = f'{SCHEMA_BASE}/{DATABASE_B}/{EVENT_TYPE}_{TABLE}_schema'
checkpoint_location_bronze = f'{CHECKPOINT_BASE}/{DATABASE_B}/{EVENT_TYPE}_{TABLE}_checkpoint'
base_path_input = f'dbfs:/Volumes/{CATALOG}/{DATABASE_L}/all_games/*/{EVENT_TYPE}/*'
file_path_input = f'{TABLE}.csv'

# Configuration variables
print(f"Bronze Table: {table_bronze}")
print(f"Bronze Schema Location: {schema_location_bronze}")
print(f"Bronze Checkpoint Location: {checkpoint_location_bronze}")
print(f"Landing Path: {base_path_input}/{file_path_input}")

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC #### Code Description
# MAGIC The code below reads data as a stream from cloud storage in a structured streaming manner. Specifically, it reads the file(s) in a given path that match a given file name using the cloudFiles input source. The base_path_input and file_path_input variables are used to construct the path to the files. The schema for the dataframe is inferred from the data. The schema location and schema evolution mode parameters are set to define the behavior if the schema changes between file submissions. The DELIMITER parameter specifies the delimiter character to be used to separate columns in the CSV file. Finally, the withColumn function is used to add two new columns to the dataframe which contain metadata about the input_file and input_event.

# COMMAND ----------

# Ingest the dataset from the landing zone as an Autoloader stream
df = (
    spark.readStream.format("cloudFiles")
    .option("cloudFiles.format", "csv")
    .option("cloudFiles.schemaLocation", schema_location_bronze)
    .option("cloudFiles.schemaEvolutionMode", "rescue")
    .option("cloudFiles.inferColumnTypes", True)
    .option("inferSchema", True)
    .option("mergeSchema", True)
    .option("header", True)
    .option("delimiter", DELIMITER)
    .load(f'{base_path_input}/{file_path_input}')
    .withColumn("input_file", f.input_file_name())
    .withColumn("input_event", udf_extract_metadata("input_file", f.lit("input_event")))
)

# COMMAND ----------

# Extract the schema for the dataframe to be used in the zipWithIndex and add a new field to it called "row_index"
schema_bronze = StructType([StructField("row_index", LongType(), True)] + df.schema.fields)

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC #### Code Description
# MAGIC The code below defines a function named add_row_index that takes in a Spark DataFrame and a batch identifier as arguments. The function adds a new column to the dataframe called row_number, which is populated with row indices that start from 1 and are uniquely assigned by partitioning the rows of the dataframe based on the input_event column and ordering them based on their row_index. Before this ordering can happen, a new dataframe is generated by zipping the original Spark strings with index, which is because the original dataframe did not include a column that can be used for ordering.
# MAGIC
# MAGIC Finally, the resulting dataframe, df_zipped, is saved as a table by writing it to a Delta table. The mode option is set to 'append', so each write to the table is appended to the existing data in the table. The option mergeSchema is set to True to automatically adjust the schema of the table based on the schema of the written dataframe. The final step is to call saveAsTable on the dataframe and provide the name of the Delta table to which the dataframe is saved.

# COMMAND ----------

def add_row_index(df, batch_id):
  """
  Function to add row indices to motion_tracker_frame_result_parameters
  to facilitate an easy stream-stream join with the motion_sequence table

  # Write the transformed DataFrame to the database using foreachBatch
  .withColumn("row_number", f.row_number().over(Window.partitionBy('input_event').orderBy("Timestamp")))
  query = transformedDF.writeStream.foreachBatch(write_to_database).start()
  """
  # We don't have a column to order by, hence we first need to zipWithIndex and then Window
  df_zipped = (
    df.rdd.zipWithIndex().map(lambda row: (row[1],) + row[0]).toDF(schema_bronze)
    .withColumn('row_number', f.row_number().over(Window.partitionBy('input_event').orderBy("row_index")))
  )
  
  # Write the data to a bronze Delta table
  (
    df_zipped
    .write.format("delta")
    .mode("append")
    .option("mergeSchema", True)
    .saveAsTable(table_bronze)
  )

# COMMAND ----------

# MAGIC %md
# MAGIC
# MAGIC #### Code Description
# MAGIC The code below starts a streaming query to write the transformed data of a dataframe to the database using the foreachBatch method. The foreachBatch method allows you to specify a function that is executed on the output data of every micro-batch of a structured streaming query. In this case, the add_row_index function is called on each micro-batch of transformed dataframes. The option method is used to pass the checkpoint location to the query to allow for fault tolerance and data recovery from a checkpoint location on failure.
# MAGIC
# MAGIC The trigger method is used to set the triggering policy for the query, which is throttled to execute as soon as possible with the option availableNow. Finally, the start method is called on the output stream to start running the query. This produces a continuous "stream" of data that can be written to the database in append mode through the add_row_index function.

# COMMAND ----------

# Write the transformed DataFrame to the database using foreachBatch
(
  df
  .writeStream
  .foreachBatch(add_row_index)
  .option("checkpointLocation", checkpoint_location_bronze)
  .trigger(availableNow=True)
  .start()
)
